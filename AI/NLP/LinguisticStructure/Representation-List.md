[![返回目录](https://user-images.githubusercontent.com/5803001/38079637-ff0abcf0-3371-11e8-9b76-ad651620afc7.jpg)](https://github.com/wx-chevalier/Awesome-Lists) 
 
# Document Representation List

- [2016-NLP Research Lab Part 1: Distributed Representations](http://blog.districtdatalabs.com/nlp-research-lab-part-1-distributed-representations): How I Learned To Stop Worrying And Love Word Embeddings

# Word Vectors

- [2018-Understanding word vectors](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469): Understanding word vectors: A tutorial for "Reading and Writing Electronic Text," a class I teach at ITP.

# Word2Vec

- [The Code Word2Vec Tutorial Part I: The SkipGram Model](http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf)，[Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

- [word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)

- [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

- [论文翻译章节：基于Negative Sampling 的模型 ](http://blog.csdn.net/itplus/article/details/37998797)

- [word2vec: negative sampling (in layman term)?](http://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term)
